---
title: "부동산 매매가격 추이 분석"
author: "taehoon"
date: "2017년 1월 31일"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 부동산 매매가격 추이 분석

해당 분석의 주된 목적은 R을 이용한 정량적 데이터 분석에 대한 이해를 도모하고 평소 관심을 가졌던 부동산 매매가격에 대한 추이를 분석하는데 있다.
부동산은 살아가면서 특히 중요한 의사결정이 필요한 분야이다. 최근에는 다방이나 직방 같은 어플리케이션이 제공되며 부동산에 대한 정보를 제공해주고 있다. 이러한 정보는 객관적인 데이터를 제공하여 사람들의 의사결정을 도와 주고 주관적인 의사결정에 도움을 주고 있다. 이러한 맥락에서 이번 분석을 통한 매매가격 분석은 부동산을 사고 팔때 도움을 줄 수 있을 것이라 생각한다.
이 분석에 큰 도움이 된 글은 전희원 님의 "R을 이용한 부동산 데이터 분석 케이스 스터디"임을 밝힌다.

## 1. 데이터 수집 및 처리 
먼저, 분석을 위해 데이터가 필요하다. 어디에서 데이터를 어떤 방법으로 수집 할 수 있는지 구글링이나 네이버를 통해 찾아보았다.
처음에 찾은 데이터는 국토교통부에서 제공하는 [실거래가 공개시스템](http://rtdown.molit.go.kr/)에서 해당 데이터에 대한 엑셀 파일을 다운로드 받을 수 있었다. 그러나 엑셀로 데이터를 다운받을 수 있는 날짜 범위가 최대 한달 밖에 되지 않아 너무 수동적으로 데이터를 수집해야 하는 문제점이 있었다. 다른 방법을 찾다가 공공데이터포털에서 제공되는 API를 찾을 수 있었다.[공공데이터포털링크](https://www.data.go.kr/dataset/3050988/openapi.do) 이 방법에도 제한 사항이 있었지만 앞에 방법보다는 더 좋아 보인다.

###1.1 데이터 수집 

- 필요한 라이브러리 추가

 - XML : API에서 xml 데이터 수집을 위한 패키지
 - data.table : data.frame 보다 더 빠르고 효율적인 data.table 사용을 위한  패키지
 - stringr : 문자열 처리 및 파이프 연산자 사용을 위한 패키기
 - ggplot2 : 시각화를 위한 패키지
```{r}
library(XML)
library(data.table)
library(stringr)
library(ggplot2)
```

- API 접근을 위한 변수 설정

- service_key : 공공데이터 포털에서 API 사용 신청 시 받는 키값
- locCode, datelist : API 접근 시 사용되는 매개변수
- 공공데이터 포털에서 일반용 API 신청시 1일 1000번 이상 접근 제한이 있음. 따라서 많은 데이터를 수집하고 싶으면 datelist의 양을 조절해야 함. 여기서는 2013~2016년 까지 총 4년간의 서울시 아파트 매매 데이터를 수집했다.

```{r}
service_key <- "EeBjN2xdCzzcqHvefO0rZXaycAim0uGpKxnOX72PY1UpkSZnifzIK1kxLm61XXaQ4pFxhbW%2F%2FZbmQDKFiAFNVA%3D%3D"
#서울시 지역코드
locCode <-c("11110","11140","11170","11200","11215","11230","11260","11290","11305","11320","11350","11380","11410","11440","11470","11500","11530","11545","11560","11590","11620","11650","11680","11710","11740")
locCode_nm <-c("종로구","중구","용산구","성동구","광진구","동대문구","중랑구","성북구","강북구","도봉구","노원구","은평구","서대문구","마포구","양천구","강서구","구로구","금천구","영등포구","동작구","관악구","서초구","강남구","송파구","강동구")
datelist <-c("201501","201502","201503","201504","201505","201506","201507","201508","201509","201510","201511","201512","201401","201402","201403","201404","201405","201406","201407","201408","201409","201410","201411","201412","201601","201602","201603","201604","201605","201606","201607","201608","201609","201610","201611","201612")
#datelist <-c("201301","201302","201303","201304","201305","201306","201307","201308","201309","201310","201311","201312")
```

- 데이터 수집할 url list 작성

```{r, eval=FALSE}
urllist <- list()
cnt <-0
for(i in 1:length(locCode)){
  for(j in 1:length(datelist)){
    cnt=cnt+1
    urllist[cnt] <-paste0("http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?LAWD_CD=",locCode[i],"&DEAL_YMD=",datelist[j],"&serviceKey=",service_key) 
  }
}

```

- 데이터 수집 과정

```{r, eval=FALSE}
total<-list()
for(i in 1:length(urllist)){
  item <- list()
  item_temp_dt<-data.table()
  raw.data <- xmlTreeParse(urllist[i], useInternalNodes = TRUE,encoding = "utf-8")
  rootNode <- xmlRoot(raw.data)
  items <- rootNode[[2]][['items']]
 # price','con_year','year','dong','aptnm','month','dat','area','bungi','loc','floor'
  size <- xmlSize(items)
  for(i in 1:size){
     item_temp <- xmlSApply(items[[i]],xmlValue)
     item_temp_dt <- data.table(price=item_temp[1],
                                con_year=item_temp[2],
                                year=item_temp[3],
                                dong=item_temp[4],
                                aptnm=item_temp[5],
                                month=item_temp[6],
                                dat=item_temp[7],
                                area=item_temp[8],
                                bungi=item_temp[9],
                                loc=item_temp[10],
                                floor=item_temp[11])
     item[[i]]<-item_temp_dt
  }
  total[[i]]<-rbindlist(item)
}
result_apt_data <- rbindlist(total)
save(result_apt_data, file="apt_item_sales_dt.Rdata")
```

###1.2 데이터 전처리

-데이터 로드 

2014~2016년도 데이터 수집한 뒤 2013년도 데이터를 수집할 시 1일 1000회 트래픽 제한을 초과하므로 하루 뒤에 2013년도 데이터를 수집하였다.  

```{r}
result_apt_data1<-get(load("apt_item_sales_dt.Rdata")) #load 2014~2016년도 데이터
result_apt_data2<-get(load("apt_item_sales_dt2.Rdata")) #load 2013년도 데이터
result_apt_data3<-get(load("apt_item_sales_dt3.Rdata")) #2011~2012년도 데이터 데이터
```

-데이터 전처리

일단 2013년 데이터와 2014~2016년 데이터를 합친 뒤 데이터 전처리 과정을 거친다.
```{r, results="hide"}
apt_data1<-data.table(result_apt_data1)
apt_data2<-data.table(result_apt_data2)
apt_data3<-data.table(result_apt_data3)
apt_data<-rbindlist(list(apt_data1,apt_data2,apt_data3))
# 데이터 내 결측 값 확인
colSums(is.na(apt_data)) 

#loc가 잘못 들어가 있는 데이터에 대한 처리
index_na <-is.na(apt_data$floor)
apt_data[index_na]$floor <- apt_data[index_na]$loc
apt_data[index_na]$loc <- apt_data[index_na]$bungi
apt_data[index_na]$bungi <- NA

#컬럼 속성 수정 및 필요한 컬럼 생성
apt_data[,price:=as.character(price)%>%str_trim()%>%sub(",","",.)%>%as.numeric()]
apt_data[,con_year:=as.character(con_year)%>%str_trim()%>%as.numeric()]
apt_data[,year:=as.character(year)%>%str_trim()%>%as.numeric()]
apt_data[between(as.numeric(as.character(month)),1,3),qrt:='Q1']
apt_data[between(as.numeric(as.character(month)),4,6),qrt:='Q2']
apt_data[between(as.numeric(as.character(month)),7,9),qrt:='Q3']
apt_data[between(as.numeric(as.character(month)),10,12),qrt:='Q4']
apt_data[,dong:=as.character(dong)%>%str_trim()]
apt_data[,yyyyqrt:=paste0(year,qrt)]
apt_data[,month:=as.character(month)%>%str_trim()%>%as.numeric()]
apt_data[,yyyym:=paste0(year,month)]
lapply(locCode,function(x){apt_data[loc==x,gu:=locCode_nm[which(locCode==x)]]})

```


## 2. 데이터 분석 

데이터 수집 및 전처리 과정을 통해 정제된 데이터를 분석합니다.
먼저 서울 지역 평균 매매가 추이를 살펴본다.
```{r, fig.cap="서울 지역 평균 매매가 추이"}
apt_data_seo_price <- aggregate(apt_data$price,by=list(apt_data$yyyyqrt),mean)
names(apt_data_seo_price) <- c("yyyyqrt","price")
ggplot(apt_data_seo_price,aes(x=yyyyqrt,y=price,group=1))+
  geom_line() +
  xlab("분기별")+
  ylab("평균매매가격")+
  stat_smooth(method='lm')
```

서울시 구/분기별 아파트 매매 가격

```{r}
apt_data_by_gu_qrt <- aggregate(apt_data$price, by=list(apt_data$yyyyqrt,apt_data$gu), mean)
names(apt_data_by_gu_qrt) <- c('yyyyqrt','gu','pricemean')

ggplot(apt_data_by_gu_qrt,aes(x=yyyyqrt,y=pricemean,group=gu))+
  geom_line()+
  facet_wrap(~gu,scale='free_y', ncol=3)+
  stat_smooth(method="lm")
```

시계열 분석
시계열에 패턴이 존재한다면 미래에도 이러한 패턴이 계속 될 것이라는 가정하에 예측을 해볼 수 있다.
그렇다면 시계열에 패턴이 존재하는지 확인하는 방법은 시계열이 랜덤한지를 알아보는 랜덤성 검증이 있다.
랜덤성 검정 -> 런검정 

```{r}
gu_meanprice <- aggregate(apt_data$price,by=list(apt_data$yyyym,apt_data$gu),mean)
head(gu_meanprice)
names(gu_meanprice)<- c('yyyym','gu','price')
#중복없이 구 추출
gu_list <- unique(gu_meanprice$gu)

# 각 구별 매매가격의 랜덤성 검정 결과를 runs_p변수에 추가
install.packages("lawstat")
library(lawstat)
runs_p<-c()
for(g in gu_list){
  runs_p <- c(runs_p, runs.test(apt_data[gu %in% g,price])$p.value)
}

ggplot(data.table(gu_list, runs_p), aes(x=gu_list, y=runs_p, group=1)) +
  geom_line() + geom_point() +
  ylab('P-value') + xlab('구')

which(runs_p>0.05)
```

모든 구에 대해서 p값이 0.05 보다 작으므로 귀무가설 (랜덤하다)기각 할 수 있다 
따라서 랜덤하지 않은 어떤 패턴이 있다고 할 수 있다.

그럼 이중에서 내가 살고 있는 관악구를 추출하여 시계열 패턴을 그려보겠다.

```{r}
gwanak_data <- apt_data[gu=="관악구",]
gwanak_price <- aggregate(gwanak_data$price,by=list(gwanak_data$yyyyqrt),mean) 
names(gwanak_price)<-c('yyyyqrt','price')
tot_ts <- ts(gwanak_price$price,start=c(2011,1),frequency = 4)
plot(tot_ts)
```

시계열 분석
```{r}
install.packages("forecast")
library(forecast)
arima_mdl <- auto.arima(tot_ts)
tsdiag(arima_mdl)

accuracy(arima_mdl)
plot(forecast(arima_mdl,h=6))
```

